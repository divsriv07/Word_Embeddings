{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 3**\n",
        "\n",
        "These steps will be added later\n"
      ],
      "metadata": {
        "id": "edtmq6CGY_vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVD**"
      ],
      "metadata": {
        "id": "n6tG2tcyRHuN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egVnL6vHQuhm",
        "outputId": "be615bf0-4768-4770-cd14-bc2ffa2c15c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing Brown corpus...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building vocabulary...\n",
            "Vocabulary size: 10000\n",
            "Building co-occurrence matrix...\n",
            "Co-occurrence matrix built.\n",
            "Fitting SVD...\n",
            "Performing SVD on CPU...\n",
            "SVD complete.\n",
            "Saved embeddings to 'svd.pt'.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "svd.py\n",
        "------\n",
        "\n",
        "Builds a co-occurrence matrix from the preprocessed Brown Corpus and applies SVD\n",
        "to generate static word embeddings. This script:\n",
        "\n",
        "1. Preprocesses the Brown corpus with:\n",
        "   - Lowercasing, stopword removal, lemmatization, rare word removal.\n",
        "   - Padding short sentences and chunking long ones.\n",
        "2. Uses the top-K most frequent words (default: 10,000).\n",
        "3. Runs SVD on CPU (to avoid memory issues on GPU).\n",
        "\n",
        "Saves the resulting embeddings to 'svd.pt'.\n",
        "\n",
        "Usage:\n",
        "    python svd.py\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.corpus import brown, stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "def downloadNltkResources():\n",
        "    \"\"\"\n",
        "    Downloads the necessary NLTK resources: brown, stopwords, and wordnet.\n",
        "    \"\"\"\n",
        "    nltk.download('brown')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "def padSentence(sentenceTokens, targetLength, padToken=\"<PAD>\"):\n",
        "    \"\"\"\n",
        "    Pads a tokenized sentence to `targetLength` by adding <PAD> tokens on both sides.\n",
        "\n",
        "    Args:\n",
        "        sentenceTokens (list of str): The tokenized sentence.\n",
        "        targetLength (int): Desired length after padding.\n",
        "        padToken (str): Token used for padding.\n",
        "\n",
        "    Returns:\n",
        "        list of str: The padded sentence.\n",
        "    \"\"\"\n",
        "    paddingNeeded = max(0, targetLength - len(sentenceTokens))\n",
        "    leftPad = paddingNeeded // 2\n",
        "    rightPad = paddingNeeded - leftPad\n",
        "    return [padToken] * leftPad + sentenceTokens + [padToken] * rightPad\n",
        "\n",
        "def chunkSentence(sentenceTokens, chunkSize, padToken=\"<PAD>\"):\n",
        "    \"\"\"\n",
        "    Splits a sentence into overlapping chunks of size `chunkSize`.\n",
        "    If the sentence is shorter than chunkSize, it is padded instead.\n",
        "\n",
        "    Args:\n",
        "        sentenceTokens (list of str): Tokenized sentence.\n",
        "        chunkSize (int): Desired chunk size.\n",
        "        padToken (str): Token used for padding.\n",
        "\n",
        "    Returns:\n",
        "        list of list of str: A list of chunked or padded sentences.\n",
        "    \"\"\"\n",
        "    if len(sentenceTokens) <= chunkSize:\n",
        "        return [padSentence(sentenceTokens, chunkSize, padToken)]\n",
        "    return [sentenceTokens[i : i + chunkSize]\n",
        "            for i in range(len(sentenceTokens) - chunkSize + 1)]\n",
        "\n",
        "def preprocessBrownCorpus(minFreq=5, windowSize=5, maxChunkSize=11):\n",
        "    \"\"\"\n",
        "    Preprocesses the Brown corpus by:\n",
        "      1. Downloading necessary NLTK resources.\n",
        "      2. Tokenizing, lowercasing, removing stopwords, and lemmatizing tokens.\n",
        "      3. Padding short sentences and chunking longer ones.\n",
        "      4. Removing words below minFreq from the final corpus.\n",
        "\n",
        "    Args:\n",
        "        minFreq (int): Minimum frequency for a word to remain in the dataset.\n",
        "        windowSize (int): Context window size (used to decide how to pad).\n",
        "        maxChunkSize (int): Maximum size of sentence chunks.\n",
        "\n",
        "    Returns:\n",
        "        list of list of str: The preprocessed tokenized sentences.\n",
        "    \"\"\"\n",
        "    downloadNltkResources()\n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    padToken = \"<PAD>\"\n",
        "    cleanedSentences = []\n",
        "\n",
        "    # Determine how many tokens we need to accommodate (context window * 2 + 1 for target)\n",
        "    targetLength = windowSize * 2 + 1\n",
        "\n",
        "    # Step 1: Read Brown corpus and tokenize\n",
        "    for rawSentence in brown.sents():\n",
        "        # Lowercase, remove stopwords, remove non-alpha, and lemmatize\n",
        "        tokens = [\n",
        "            lemmatizer.lemmatize(word.lower())\n",
        "            for word in rawSentence\n",
        "            if word.isalpha() and word.lower() not in stopWords\n",
        "        ]\n",
        "\n",
        "        if not tokens:\n",
        "            continue\n",
        "\n",
        "        # Step 2: Pad if shorter, or chunk if longer than maxChunkSize\n",
        "        if len(tokens) < targetLength:\n",
        "            # If smaller than the target length, pad to target length\n",
        "            padded = padSentence(tokens, targetLength, padToken)\n",
        "            cleanedSentences.append(padded)\n",
        "        elif len(tokens) > maxChunkSize:\n",
        "            # If longer, chunk into overlapping windows of size maxChunkSize\n",
        "            chunks = chunkSentence(tokens, maxChunkSize, padToken)\n",
        "            cleanedSentences.extend(chunks)\n",
        "        else:\n",
        "            # Otherwise, keep as is\n",
        "            cleanedSentences.append(tokens)\n",
        "\n",
        "    # Step 3: Filter out rare words below `minFreq`\n",
        "    wordCounts = Counter(word for sent in cleanedSentences for word in sent)\n",
        "    validWords = {\n",
        "        word for word, count in wordCounts.items()\n",
        "        if count >= minFreq or word == padToken\n",
        "    }\n",
        "\n",
        "    # Step 4: Rebuild final sentences, removing words not in validWords\n",
        "    finalSentences = [[word for word in sent if word in validWords]\n",
        "                      for sent in cleanedSentences]\n",
        "\n",
        "    return finalSentences\n",
        "\n",
        "class SVDModel:\n",
        "    \"\"\"\n",
        "    Class for building a co-occurrence matrix and performing SVD-based embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, windowSize=5, embeddingDim=100, maxVocabSize=10000):\n",
        "        \"\"\"\n",
        "        Initializes the SVDModel.\n",
        "\n",
        "        Args:\n",
        "            windowSize (int): Number of words to consider on each side of a target word.\n",
        "            embeddingDim (int): Embedding dimension for the final embeddings.\n",
        "            maxVocabSize (int): Maximum vocabulary size (top-K by frequency).\n",
        "        \"\"\"\n",
        "        self.windowSize = windowSize\n",
        "        self.embeddingDim = embeddingDim\n",
        "        self.maxVocabSize = maxVocabSize\n",
        "        self.wordToIdx = {}\n",
        "        self.idxToWord = {}\n",
        "        self.cooccurMatrix = None\n",
        "        self.vocabSize = 0\n",
        "\n",
        "    def buildVocabulary(self, sentences):\n",
        "        \"\"\"\n",
        "        Creates a truncated vocabulary from the preprocessed sentences.\n",
        "\n",
        "        Args:\n",
        "            sentences (list of list of str]): Preprocessed sentences.\n",
        "        \"\"\"\n",
        "        wordCounter = Counter(word for sent in sentences for word in sent)\n",
        "        mostCommon = wordCounter.most_common(self.maxVocabSize)\n",
        "        vocab = sorted([word for word, _ in mostCommon])\n",
        "\n",
        "        self.wordToIdx = {word: idx for idx, word in enumerate(vocab)}\n",
        "        self.idxToWord = {idx: word for word, idx in self.wordToIdx.items()}\n",
        "        self.vocabSize = len(vocab)\n",
        "        print(f\"Vocabulary size: {self.vocabSize}\")\n",
        "\n",
        "    def buildCooccurrenceMatrix(self, sentences):\n",
        "        \"\"\"\n",
        "        Builds a co-occurrence matrix using a symmetric context window.\n",
        "\n",
        "        Args:\n",
        "            sentences (list of list of str]): Preprocessed sentences.\n",
        "        \"\"\"\n",
        "        cooccurDict = defaultdict(float)\n",
        "\n",
        "        for sent in sentences:\n",
        "            length = len(sent)\n",
        "            for i, targetWord in enumerate(sent):\n",
        "                if targetWord not in self.wordToIdx:\n",
        "                    continue\n",
        "                targetIdx = self.wordToIdx[targetWord]\n",
        "                start = max(0, i - self.windowSize)\n",
        "                end = min(length, i + self.windowSize + 1)\n",
        "                for j in range(start, end):\n",
        "                    if j != i:\n",
        "                        contextWord = sent[j]\n",
        "                        if contextWord in self.wordToIdx:\n",
        "                            contextIdx = self.wordToIdx[contextWord]\n",
        "                            cooccurDict[(targetIdx, contextIdx)] += 1.0\n",
        "\n",
        "        # Initialize the co-occurrence matrix\n",
        "        self.cooccurMatrix = torch.zeros(self.vocabSize, self.vocabSize)\n",
        "        for (i, j), val in cooccurDict.items():\n",
        "            self.cooccurMatrix[i, j] = val\n",
        "\n",
        "        print(\"Co-occurrence matrix built.\")\n",
        "\n",
        "    def fitSVD(self):\n",
        "        \"\"\"\n",
        "        Performs SVD on the co-occurrence matrix and returns the resulting embeddings.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The computed word embeddings of shape [vocabSize, embeddingDim].\n",
        "        \"\"\"\n",
        "        print(\"Performing SVD on CPU...\")\n",
        "        U, S, _ = torch.svd(self.cooccurMatrix.cpu())\n",
        "        sqrtS = torch.diag(torch.sqrt(S))\n",
        "\n",
        "        # Keep only the top embeddingDim components\n",
        "        embeddings = torch.mm(U[:, :self.embeddingDim], sqrtS[:self.embeddingDim, :self.embeddingDim])\n",
        "        print(\"SVD complete.\")\n",
        "        return embeddings\n",
        "\n",
        "def main():\n",
        "    print(\"Preprocessing Brown corpus...\")\n",
        "    sentences = preprocessBrownCorpus(\n",
        "        minFreq=5,   # minimum frequency\n",
        "        windowSize=5,\n",
        "        maxChunkSize=11\n",
        "    )\n",
        "\n",
        "    svdModel = SVDModel(\n",
        "        windowSize=5,\n",
        "        embeddingDim=100,\n",
        "        maxVocabSize=10000\n",
        "    )\n",
        "\n",
        "    print(\"Building vocabulary...\")\n",
        "    svdModel.buildVocabulary(sentences)\n",
        "\n",
        "    print(\"Building co-occurrence matrix...\")\n",
        "    svdModel.buildCooccurrenceMatrix(sentences)\n",
        "\n",
        "    print(\"Fitting SVD...\")\n",
        "    embeddings = svdModel.fitSVD()\n",
        "\n",
        "    saveDict = {\n",
        "        'embeddings': embeddings,\n",
        "        'wordToIdx': svdModel.wordToIdx,\n",
        "        'idxToWord': svdModel.idxToWord\n",
        "    }\n",
        "    torch.save(saveDict, 'svd.pt')\n",
        "    print(\"Saved embeddings to 'svd.pt'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CBOW**"
      ],
      "metadata": {
        "id": "OxscHhOdUwuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "cbow.py\n",
        "-------\n",
        "\n",
        "Implements the Continuous Bag-of-Words (CBOW) model with Negative Sampling using PyTorch.\n",
        "This script:\n",
        "\n",
        "1. Preprocesses the Brown corpus by:\n",
        "   - Lowercasing, stopword removal, lemmatization, rare word removal, etc.\n",
        "2. Builds a vocabulary (top-K words by frequency).\n",
        "3. Trains the CBOW model and saves the learned embeddings to 'cbow.pt'.\n",
        "\n",
        "Usage:\n",
        "    python cbow.py\n",
        "\"\"\"\n",
        "\n",
        "import nltk\n",
        "import torch\n",
        "from torch import nn\n",
        "from nltk.corpus import brown, stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "\n",
        "# ------------------------ Preprocessing Helpers ------------------------ #\n",
        "\n",
        "def downloadNltkResources():\n",
        "    \"\"\"\n",
        "    Downloads the necessary NLTK resources: brown, stopwords, and wordnet.\n",
        "    \"\"\"\n",
        "    nltk.download('brown')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "def filterAndLemmatizeSentence(rawSentence, stopWords, lemmatizer):\n",
        "    \"\"\"\n",
        "    Converts a raw sentence to lowercase, removes stopwords and non-alpha tokens,\n",
        "    and then lemmatizes the remaining tokens.\n",
        "\n",
        "    Args:\n",
        "        rawSentence (list of str): Original sentence tokens from Brown corpus.\n",
        "        stopWords (set of str): Set of stopwords to remove.\n",
        "        lemmatizer (WordNetLemmatizer): NLTK lemmatizer instance.\n",
        "\n",
        "    Returns:\n",
        "        list of str: The processed tokens.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        lemmatizer.lemmatize(word.lower())\n",
        "        for word in rawSentence\n",
        "        if word.isalpha() and word.lower() not in stopWords\n",
        "    ]\n",
        "\n",
        "def preprocessBrownCorpus(\n",
        "    maxChunkSize=50,\n",
        "    minFreq=5,\n",
        "    topVocab=10000\n",
        "):\n",
        "    \"\"\"\n",
        "    Preprocesses the Brown corpus by:\n",
        "      1. Downloading necessary NLTK resources.\n",
        "      2. Filtering and lemmatizing tokens (stopwords removed).\n",
        "      3. (Optionally) chunking or padding (You can add if needed).\n",
        "      4. Removing words below a minimum frequency threshold.\n",
        "      5. Limiting the vocabulary to top 'topVocab' words.\n",
        "\n",
        "    Args:\n",
        "        maxChunkSize (int): An optional parameter if you want to chunk very long sentences.\n",
        "        minFreq (int): Minimum frequency for a word to remain in the dataset.\n",
        "        topVocab (int): Maximum vocabulary size (top-K words).\n",
        "\n",
        "    Returns:\n",
        "        list of list of str: The preprocessed tokenized sentences.\n",
        "        dict: A mapping from word -> frequency (for building negative sampling distribution).\n",
        "    \"\"\"\n",
        "    downloadNltkResources()\n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    processedSentences = []\n",
        "\n",
        "    # Step 1: Read the Brown corpus and preprocess each sentence\n",
        "    for rawSentence in brown.sents():\n",
        "        tokens = filterAndLemmatizeSentence(rawSentence, stopWords, lemmatizer)\n",
        "        if tokens:\n",
        "            # Optional: chunk or pad if needed. For now, we just keep the tokens.\n",
        "            # If a sentence is extremely long, you might want to chunk it:\n",
        "            if len(tokens) > maxChunkSize:\n",
        "                # Example chunking in fixed-size blocks\n",
        "                for i in range(0, len(tokens), maxChunkSize):\n",
        "                    processedSentences.append(tokens[i : i + maxChunkSize])\n",
        "            else:\n",
        "                processedSentences.append(tokens)\n",
        "\n",
        "    # Step 2: Remove words below minFreq\n",
        "    fullCounter = Counter(word for sent in processedSentences for word in sent)\n",
        "    filteredWords = {w for w, c in fullCounter.items() if c >= minFreq}\n",
        "\n",
        "    filteredSentences = [\n",
        "        [w for w in sent if w in filteredWords]\n",
        "        for sent in processedSentences\n",
        "    ]\n",
        "\n",
        "    # Step 3: Build top vocabulary list by frequency\n",
        "    freqCounter = Counter(word for sent in filteredSentences for word in sent)\n",
        "    mostCommon = freqCounter.most_common(topVocab)\n",
        "    vocabList = sorted(w for w, _ in mostCommon)  # final vocab\n",
        "\n",
        "    # Step 4: Re-filter sentences to keep only words in the final top vocab\n",
        "    finalSentences = [\n",
        "        [w for w in sent if w in vocabList]\n",
        "        for sent in filteredSentences\n",
        "    ]\n",
        "\n",
        "    # Remove empty sentences\n",
        "    finalSentences = [sent for sent in finalSentences if len(sent) > 0]\n",
        "\n",
        "    return finalSentences, freqCounter\n",
        "\n",
        "# ------------------------ CBOW Dataset & Model ------------------------ #\n",
        "\n",
        "class CBOWDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for CBOW training. Each sample consists of a list of context word indices\n",
        "    and a target word index.\n",
        "    \"\"\"\n",
        "    def __init__(self, sentences, wordToIdx, windowSize=2):\n",
        "        \"\"\"\n",
        "        Initializes the CBOW dataset.\n",
        "\n",
        "        Args:\n",
        "            sentences (list of list of str]): Tokenized, preprocessed sentences.\n",
        "            wordToIdx (dict): Mapping from words to indices.\n",
        "            windowSize (int): Size of the context window on each side.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.windowSize = windowSize\n",
        "        self.wordToIdx = wordToIdx\n",
        "        self.data = []\n",
        "\n",
        "        for sent in sentences:\n",
        "            # Make sure we only use words in our vocabulary\n",
        "            filteredSent = [word for word in sent if word in self.wordToIdx]\n",
        "            if len(filteredSent) < 2 * windowSize + 1:\n",
        "                continue\n",
        "\n",
        "            for i in range(windowSize, len(filteredSent) - windowSize):\n",
        "                contextIndices = []\n",
        "                for j in range(i - windowSize, i + windowSize + 1):\n",
        "                    if j != i:\n",
        "                        contextIndices.append(self.wordToIdx[filteredSent[j]])\n",
        "                targetIndex = self.wordToIdx[filteredSent[i]]\n",
        "                self.data.append((contextIndices, targetIndex))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context, target = self.data[idx]\n",
        "        contextTensor = torch.tensor(context, dtype=torch.long)\n",
        "        targetTensor = torch.tensor(target, dtype=torch.long)\n",
        "        return (contextTensor, targetTensor)\n",
        "\n",
        "class CBOWModel(nn.Module):\n",
        "    \"\"\"\n",
        "    CBOW model with negative sampling.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocabSize, embeddingDim):\n",
        "        \"\"\"\n",
        "        Initializes the CBOW model.\n",
        "\n",
        "        Args:\n",
        "            vocabSize (int): Number of words in the vocabulary.\n",
        "            embeddingDim (int): Dimensionality of the embeddings.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.vocabSize = vocabSize\n",
        "        self.embeddingDim = embeddingDim\n",
        "\n",
        "        # Embeddings for input (context) and output (target)\n",
        "        self.inEmbedding = nn.Embedding(vocabSize, embeddingDim)\n",
        "        self.outEmbedding = nn.Embedding(vocabSize, embeddingDim)\n",
        "\n",
        "        # Initialize embeddings uniformly\n",
        "        initRange = 0.5 / embeddingDim\n",
        "        self.inEmbedding.weight.data.uniform_(-initRange, initRange)\n",
        "        self.outEmbedding.weight.data.uniform_(-initRange, initRange)\n",
        "\n",
        "    def forward(self, contextIndices, targetIndices, negativeIndices):\n",
        "        \"\"\"\n",
        "        Forward pass computes the loss using negative sampling.\n",
        "\n",
        "        Args:\n",
        "            contextIndices (Tensor): [batchSize, contextWindow] context word indices.\n",
        "            targetIndices (Tensor): [batchSize] target word indices.\n",
        "            negativeIndices (Tensor): [batchSize, numNegSamples] negative sample indices.\n",
        "\n",
        "        Returns:\n",
        "            loss (Tensor): The computed loss for the batch.\n",
        "        \"\"\"\n",
        "        # contextEmbeds: [B, contextWindow, D]\n",
        "        contextEmbeds = self.inEmbedding(contextIndices)\n",
        "        # Average: [B, D]\n",
        "        contextEmbeds = torch.mean(contextEmbeds, dim=1)\n",
        "\n",
        "        # Positive scores\n",
        "        targetEmbeds = self.outEmbedding(targetIndices)  # [B, D]\n",
        "        positiveScores = torch.sum(contextEmbeds * targetEmbeds, dim=1)  # [B]\n",
        "\n",
        "        # Negative samples: [B, numNegSamples, D]\n",
        "        negEmbeds = self.outEmbedding(negativeIndices)\n",
        "        # [B, numNegSamples]\n",
        "        negativeScores = torch.bmm(negEmbeds, contextEmbeds.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "        # Loss computation\n",
        "        positiveLoss = -torch.log(torch.sigmoid(positiveScores) + 1e-10)\n",
        "        negativeLoss = -torch.sum(torch.log(torch.sigmoid(-negativeScores) + 1e-10), dim=1)\n",
        "        loss = torch.mean(positiveLoss + negativeLoss)\n",
        "        return loss\n",
        "\n",
        "def generateNegativeSamples(batchSize, numNegSamples, vocabSize, wordFreqs):\n",
        "    \"\"\"\n",
        "    Generates negative samples based on word frequencies.\n",
        "\n",
        "    Args:\n",
        "        batchSize (int): Number of samples in the batch.\n",
        "        numNegSamples (int): Number of negative samples per instance.\n",
        "        vocabSize (int): Total number of words in the vocabulary.\n",
        "        wordFreqs (Tensor): Word frequency distribution for sampling.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Negative sample indices with shape [batchSize, numNegSamples].\n",
        "    \"\"\"\n",
        "    # Draw from wordFreqs\n",
        "    negatives = torch.multinomial(wordFreqs, batchSize * numNegSamples, replacement=True)\n",
        "    return negatives.view(batchSize, numNegSamples)\n",
        "\n",
        "# ----------------------------- Main Function ----------------------------- #\n",
        "\n",
        "def main():\n",
        "    print(\"Preprocessing Brown corpus for CBOW...\")\n",
        "    # Example hyperparameters for preprocessing\n",
        "    maxChunkSize = 50\n",
        "    minFreq = 5\n",
        "    topVocab = 10000\n",
        "\n",
        "    finalSentences, freqCounter = preprocessBrownCorpus(\n",
        "        maxChunkSize=maxChunkSize,\n",
        "        minFreq=minFreq,\n",
        "        topVocab=topVocab\n",
        "    )\n",
        "\n",
        "    # Build final vocabulary\n",
        "    vocabCounter = Counter(word for sent in finalSentences for word in sent)\n",
        "    mostCommon = vocabCounter.most_common(topVocab)\n",
        "    vocabList = sorted([word for word, _ in mostCommon])\n",
        "    wordToIdx = {word: i for i, word in enumerate(vocabList)}\n",
        "    idxToWord = {i: word for word, i in wordToIdx.items()}\n",
        "    vocabSize = len(vocabList)\n",
        "    print(f\"Vocabulary size after preprocessing: {vocabSize}\")\n",
        "\n",
        "    # Prepare negative sampling distribution\n",
        "    counts = torch.tensor([vocabCounter[w] for w in vocabList], dtype=torch.float)\n",
        "    wordFreqs = counts ** 0.75\n",
        "    wordFreqs = wordFreqs / torch.sum(wordFreqs)\n",
        "\n",
        "    # Create the CBOW dataset\n",
        "    windowSize = 2\n",
        "    dataset = CBOWDataset(finalSentences, wordToIdx, windowSize=windowSize)\n",
        "\n",
        "    dataLoader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Initialize CBOW model\n",
        "    embeddingDim = 100\n",
        "    model = CBOWModel(vocabSize, embeddingDim)\n",
        "\n",
        "    # Check for GPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Training on device: {device}\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Optimizer and training hyperparams\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    epochs = 5\n",
        "    numNegSamples = 5\n",
        "    print(\"Starting CBOW training...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        totalLoss = 0.0\n",
        "        for contextIndices, targetIndices in dataLoader:\n",
        "            contextIndices = contextIndices.to(device)\n",
        "            targetIndices = targetIndices.to(device)\n",
        "\n",
        "            negSamples = generateNegativeSamples(\n",
        "                batchSize=contextIndices.size(0),\n",
        "                numNegSamples=numNegSamples,\n",
        "                vocabSize=vocabSize,\n",
        "                wordFreqs=wordFreqs\n",
        "            ).to(device)\n",
        "\n",
        "            loss = model(contextIndices, targetIndices, negSamples)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            totalLoss += loss.item()\n",
        "\n",
        "        avgLoss = totalLoss / len(dataLoader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avgLoss:.4f}\")\n",
        "\n",
        "    # Save the trained embeddings\n",
        "    saveDict = {\n",
        "        'inEmbedding': model.inEmbedding.weight.data.cpu(),\n",
        "        'outEmbedding': model.outEmbedding.weight.data.cpu(),\n",
        "        'wordToIdx': wordToIdx,\n",
        "        'idxToWord': idxToWord\n",
        "    }\n",
        "    torch.save(saveDict, 'cbow.pt')\n",
        "    print(\"CBOW embeddings saved to 'cbow.pt'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DLFlm9pUzKz",
        "outputId": "014c65a8-79bb-4601-ce59-d1679baf8b6c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing Brown corpus for CBOW...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size after preprocessing: 10000\n",
            "Training on device: cuda\n",
            "Starting CBOW training...\n",
            "Epoch 1/5, Average Loss: 2.8530\n",
            "Epoch 2/5, Average Loss: 2.5681\n",
            "Epoch 3/5, Average Loss: 2.3535\n",
            "Epoch 4/5, Average Loss: 2.1866\n",
            "Epoch 5/5, Average Loss: 2.0362\n",
            "CBOW embeddings saved to 'cbow.pt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SkipGram**"
      ],
      "metadata": {
        "id": "aDVP2LRMWFlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "skipgram.py\n",
        "-----------\n",
        "\n",
        "Implements the Skip-Gram model with Negative Sampling using PyTorch. This script:\n",
        "\n",
        "1. Preprocesses the Brown corpus by:\n",
        "   - Lowercasing, stopword removal, lemmatization, rare word removal, etc.\n",
        "2. Builds a vocabulary (top-K words by frequency).\n",
        "3. Trains the Skip-Gram model and saves the learned embeddings to 'skipgram.pt'.\n",
        "\n",
        "Usage:\n",
        "    python skipgram.py\n",
        "\"\"\"\n",
        "\n",
        "import nltk\n",
        "import torch\n",
        "from torch import nn\n",
        "from nltk.corpus import brown, stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "\n",
        "# ------------------------ Preprocessing Helpers ------------------------ #\n",
        "\n",
        "def downloadNltkResources():\n",
        "    \"\"\"\n",
        "    Downloads the necessary NLTK resources: brown, stopwords, and wordnet.\n",
        "    \"\"\"\n",
        "    nltk.download('brown')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "def filterAndLemmatizeSentence(rawSentence, stopWords, lemmatizer):\n",
        "    \"\"\"\n",
        "    Converts a raw sentence to lowercase, removes stopwords and non-alpha tokens,\n",
        "    and then lemmatizes the remaining tokens.\n",
        "\n",
        "    Args:\n",
        "        rawSentence (list of str): Original sentence tokens from Brown corpus.\n",
        "        stopWords (set of str): Set of stopwords to remove.\n",
        "        lemmatizer (WordNetLemmatizer): NLTK lemmatizer instance.\n",
        "\n",
        "    Returns:\n",
        "        list of str: The processed tokens.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        lemmatizer.lemmatize(word.lower())\n",
        "        for word in rawSentence\n",
        "        if word.isalpha() and word.lower() not in stopWords\n",
        "    ]\n",
        "\n",
        "def preprocessBrownCorpus(\n",
        "    maxChunkSize=50,\n",
        "    minFreq=5,\n",
        "    topVocab=10000\n",
        "):\n",
        "    \"\"\"\n",
        "    Preprocesses the Brown corpus by:\n",
        "      1. Downloading necessary NLTK resources.\n",
        "      2. Filtering and lemmatizing tokens (stopwords removed).\n",
        "      3. (Optionally) chunking or splitting if sentences are very long.\n",
        "      4. Removing words below a minimum frequency threshold.\n",
        "      5. Limiting the vocabulary to top 'topVocab' words.\n",
        "\n",
        "    Args:\n",
        "        maxChunkSize (int): An optional parameter if you want to chunk very long sentences.\n",
        "        minFreq (int): Minimum frequency for a word to remain in the dataset.\n",
        "        topVocab (int): Maximum vocabulary size (top-K words).\n",
        "\n",
        "    Returns:\n",
        "        list of list of str: The preprocessed tokenized sentences.\n",
        "        dict: A mapping from word -> frequency (for building negative sampling distribution).\n",
        "    \"\"\"\n",
        "    downloadNltkResources()\n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    processedSentences = []\n",
        "\n",
        "    # Step 1: Read the Brown corpus and preprocess each sentence\n",
        "    for rawSentence in brown.sents():\n",
        "        tokens = filterAndLemmatizeSentence(rawSentence, stopWords, lemmatizer)\n",
        "        if tokens:\n",
        "            # If the sentence is extremely long, chunk it\n",
        "            if len(tokens) > maxChunkSize:\n",
        "                for i in range(0, len(tokens), maxChunkSize):\n",
        "                    processedSentences.append(tokens[i : i + maxChunkSize])\n",
        "            else:\n",
        "                processedSentences.append(tokens)\n",
        "\n",
        "    # Step 2: Remove words below minFreq\n",
        "    fullCounter = Counter(word for sent in processedSentences for word in sent)\n",
        "    filteredWords = {w for w, c in fullCounter.items() if c >= minFreq}\n",
        "\n",
        "    filteredSentences = [\n",
        "        [w for w in sent if w in filteredWords]\n",
        "        for sent in processedSentences\n",
        "    ]\n",
        "\n",
        "    # Step 3: Build top vocabulary list by frequency\n",
        "    freqCounter = Counter(word for sent in filteredSentences for word in sent)\n",
        "    mostCommon = freqCounter.most_common(topVocab)\n",
        "    vocabList = sorted(w for w, _ in mostCommon)  # final vocab\n",
        "\n",
        "    # Step 4: Re-filter sentences to keep only words in the final top vocab\n",
        "    finalSentences = [\n",
        "        [w for w in sent if w in vocabList]\n",
        "        for sent in filteredSentences\n",
        "    ]\n",
        "\n",
        "    # Remove empty sentences\n",
        "    finalSentences = [sent for sent in finalSentences if len(sent) > 0]\n",
        "\n",
        "    return finalSentences, freqCounter\n",
        "\n",
        "# ------------------------ Skip-Gram Dataset & Model ------------------------ #\n",
        "\n",
        "class SkipGramDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for Skip-Gram training. Each sample is a (center, context) word pair.\n",
        "    \"\"\"\n",
        "    def __init__(self, sentences, wordToIdx, windowSize=2):\n",
        "        \"\"\"\n",
        "        Initializes the Skip-Gram dataset.\n",
        "\n",
        "        Args:\n",
        "            sentences (list of list of str]): Tokenized, preprocessed sentences.\n",
        "            wordToIdx (dict): Mapping from words to indices.\n",
        "            windowSize (int): Size of the context window on each side.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.windowSize = windowSize\n",
        "        self.wordToIdx = wordToIdx\n",
        "        self.data = []\n",
        "\n",
        "        for sent in sentences:\n",
        "            # Filter to ensure all words are in vocabulary\n",
        "            filteredSent = [word for word in sent if word in self.wordToIdx]\n",
        "            if len(filteredSent) < 2:\n",
        "                continue\n",
        "\n",
        "            # For each word in the sentence, treat it as center and gather context\n",
        "            for i, centerWord in enumerate(filteredSent):\n",
        "                centerIdx = self.wordToIdx[centerWord]\n",
        "                start = max(0, i - windowSize)\n",
        "                end = min(len(filteredSent), i + windowSize + 1)\n",
        "                for j in range(start, end):\n",
        "                    if j != i:\n",
        "                        contextIdx = self.wordToIdx[filteredSent[j]]\n",
        "                        self.data.append((centerIdx, contextIdx))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a tuple (centerIndex, contextIndex) as tensors.\n",
        "        \"\"\"\n",
        "        center, context = self.data[idx]\n",
        "        centerTensor = torch.tensor(center, dtype=torch.long)\n",
        "        contextTensor = torch.tensor(context, dtype=torch.long)\n",
        "        return (centerTensor, contextTensor)\n",
        "\n",
        "class SkipGramModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Skip-Gram model with Negative Sampling.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocabSize, embeddingDim):\n",
        "        \"\"\"\n",
        "        Initializes the Skip-Gram model.\n",
        "\n",
        "        Args:\n",
        "            vocabSize (int): Number of words in the vocabulary.\n",
        "            embeddingDim (int): Dimensionality of the embeddings.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.vocabSize = vocabSize\n",
        "        self.embeddingDim = embeddingDim\n",
        "\n",
        "        # inEmbedding -> center word embedding\n",
        "        # outEmbedding -> context (target) word embedding\n",
        "        self.inEmbedding = nn.Embedding(vocabSize, embeddingDim)\n",
        "        self.outEmbedding = nn.Embedding(vocabSize, embeddingDim)\n",
        "\n",
        "        initRange = 0.5 / embeddingDim\n",
        "        self.inEmbedding.weight.data.uniform_(-initRange, initRange)\n",
        "        self.outEmbedding.weight.data.uniform_(-initRange, initRange)\n",
        "\n",
        "    def forward(self, centerIndices, contextIndices, negativeIndices):\n",
        "        \"\"\"\n",
        "        Forward pass computes the Skip-Gram loss using negative sampling.\n",
        "\n",
        "        Args:\n",
        "            centerIndices (Tensor): [batchSize] center word indices.\n",
        "            contextIndices (Tensor): [batchSize] context word indices.\n",
        "            negativeIndices (Tensor): [batchSize, numNegSamples] negative word indices.\n",
        "\n",
        "        Returns:\n",
        "            loss (Tensor): Computed loss for the batch.\n",
        "        \"\"\"\n",
        "        # centerEmbeds: [B, D]\n",
        "        centerEmbeds = self.inEmbedding(centerIndices)\n",
        "        # contextEmbeds: [B, D]\n",
        "        contextEmbeds = self.outEmbedding(contextIndices)\n",
        "        # Positive scores\n",
        "        positiveScores = torch.sum(centerEmbeds * contextEmbeds, dim=1)\n",
        "\n",
        "        # Negative samples: [B, numNegSamples, D]\n",
        "        negEmbeds = self.outEmbedding(negativeIndices)\n",
        "        # Dot products: [B, numNegSamples]\n",
        "        negativeScores = torch.bmm(negEmbeds, centerEmbeds.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "        # Compute losses\n",
        "        positiveLoss = -torch.log(torch.sigmoid(positiveScores) + 1e-10)\n",
        "        negativeLoss = -torch.sum(\n",
        "            torch.log(torch.sigmoid(-negativeScores) + 1e-10), dim=1\n",
        "        )\n",
        "        loss = torch.mean(positiveLoss + negativeLoss)\n",
        "        return loss\n",
        "\n",
        "def generateNegativeSamples(batchSize, numNegSamples, vocabSize, wordFreqs):\n",
        "    \"\"\"\n",
        "    Generates negative samples for Skip-Gram based on word frequencies.\n",
        "\n",
        "    Args:\n",
        "        batchSize (int): Number of samples in the batch.\n",
        "        numNegSamples (int): Number of negative samples per instance.\n",
        "        vocabSize (int): Total number of words in the vocabulary.\n",
        "        wordFreqs (Tensor): Word frequency distribution.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Negative sample indices with shape [batchSize, numNegSamples].\n",
        "    \"\"\"\n",
        "    negatives = torch.multinomial(wordFreqs, batchSize * numNegSamples, replacement=True)\n",
        "    return negatives.view(batchSize, numNegSamples)\n",
        "\n",
        "# ----------------------------- Main Function ----------------------------- #\n",
        "\n",
        "def main():\n",
        "    print(\"Preprocessing Brown corpus for Skip-Gram...\")\n",
        "    # Example hyperparameters for preprocessing\n",
        "    maxChunkSize = 50\n",
        "    minFreq = 5\n",
        "    topVocab = 10000\n",
        "\n",
        "    finalSentences, freqCounter = preprocessBrownCorpus(\n",
        "        maxChunkSize=maxChunkSize,\n",
        "        minFreq=minFreq,\n",
        "        topVocab=topVocab\n",
        "    )\n",
        "\n",
        "    # Build final vocabulary\n",
        "    vocabCounter = Counter(word for sent in finalSentences for word in sent)\n",
        "    mostCommon = vocabCounter.most_common(topVocab)\n",
        "    vocabList = sorted([word for word, _ in mostCommon])\n",
        "    wordToIdx = {word: i for i, word in enumerate(vocabList)}\n",
        "    idxToWord = {i: word for word, i in wordToIdx.items()}\n",
        "    vocabSize = len(vocabList)\n",
        "    print(f\"Vocabulary size after preprocessing: {vocabSize}\")\n",
        "\n",
        "    # Prepare negative sampling distribution\n",
        "    counts = torch.tensor([vocabCounter[w] for w in vocabList], dtype=torch.float)\n",
        "    wordFreqs = counts ** 0.75\n",
        "    wordFreqs = wordFreqs / torch.sum(wordFreqs)\n",
        "\n",
        "    # Create the Skip-Gram dataset\n",
        "    windowSize = 2\n",
        "    dataset = SkipGramDataset(finalSentences, wordToIdx, windowSize=windowSize)\n",
        "    dataLoader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Initialize Skip-Gram model\n",
        "    embeddingDim = 100\n",
        "    model = SkipGramModel(vocabSize, embeddingDim)\n",
        "\n",
        "    # Check for GPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Training on device: {device}\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Optimizer and training hyperparams\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    epochs = 5\n",
        "    numNegSamples = 5\n",
        "    print(\"Starting Skip-Gram training...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        totalLoss = 0.0\n",
        "        for centerIndices, contextIndices in dataLoader:\n",
        "            batchSize = centerIndices.size(0)\n",
        "            centerIndices = centerIndices.to(device)\n",
        "            contextIndices = contextIndices.to(device)\n",
        "\n",
        "            negSamples = generateNegativeSamples(\n",
        "                batchSize=batchSize,\n",
        "                numNegSamples=numNegSamples,\n",
        "                vocabSize=vocabSize,\n",
        "                wordFreqs=wordFreqs\n",
        "            ).to(device)\n",
        "\n",
        "            loss = model(centerIndices, contextIndices, negSamples)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            totalLoss += loss.item()\n",
        "\n",
        "        avgLoss = totalLoss / len(dataLoader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avgLoss:.4f}\")\n",
        "\n",
        "    # Save the trained embeddings\n",
        "    saveDict = {\n",
        "        'inEmbedding': model.inEmbedding.weight.data.cpu(),\n",
        "        'outEmbedding': model.outEmbedding.weight.data.cpu(),\n",
        "        'wordToIdx': wordToIdx,\n",
        "        'idxToWord': idxToWord\n",
        "    }\n",
        "    torch.save(saveDict, 'skipgram.pt')\n",
        "    print(\"Skip-Gram embeddings saved to 'skipgram.pt'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVZ_djbpWHz4",
        "outputId": "c9d0b253-1c0c-43fa-f08e-183f82952d54"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing Brown corpus for Skip-Gram...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size after preprocessing: 10000\n",
            "Training on device: cuda\n",
            "Starting Skip-Gram training...\n",
            "Epoch 1/5, Average Loss: 2.6347\n",
            "Epoch 2/5, Average Loss: 2.3579\n",
            "Epoch 3/5, Average Loss: 2.2121\n",
            "Epoch 4/5, Average Loss: 2.1088\n",
            "Epoch 5/5, Average Loss: 2.0386\n",
            "Skip-Gram embeddings saved to 'skipgram.pt'.\n"
          ]
        }
      ]
    }
  ]
}
